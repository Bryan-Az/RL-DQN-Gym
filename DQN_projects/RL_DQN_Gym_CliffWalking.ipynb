{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "91784375546a4f70b6dea96672b9d9b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ba85ffc104c433ea4ed6fa601121ebb",
              "IPY_MODEL_2901746b0c1a40dfbab1344b4e5854eb",
              "IPY_MODEL_7f2034df60fa498992a38be765f9b15c"
            ],
            "layout": "IPY_MODEL_c7f525ac5e65471f9a7c5cae3f5ff73c"
          }
        },
        "0ba85ffc104c433ea4ed6fa601121ebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea5a64625959494989cab519b49f22bb",
            "placeholder": "​",
            "style": "IPY_MODEL_fb38045edf9643f8b7fb522ad013cb10",
            "value": "100%"
          }
        },
        "2901746b0c1a40dfbab1344b4e5854eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aed28341c6734cc0a489ae1e5b46247c",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0d5b4cf648a4b748f70ee12a5bf350c",
            "value": 10000
          }
        },
        "7f2034df60fa498992a38be765f9b15c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18d4dd9192134d5994ed7cc4edd26994",
            "placeholder": "​",
            "style": "IPY_MODEL_39180ed07f924558a5668de12b6fd67e",
            "value": " 10000/10000 [00:10&lt;00:00, 1079.74step/s]"
          }
        },
        "c7f525ac5e65471f9a7c5cae3f5ff73c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea5a64625959494989cab519b49f22bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb38045edf9643f8b7fb522ad013cb10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aed28341c6734cc0a489ae1e5b46247c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0d5b4cf648a4b748f70ee12a5bf350c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18d4dd9192134d5994ed7cc4edd26994": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39180ed07f924558a5668de12b6fd67e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bryan-Az/RL-DQN-Gym/blob/main/DQN_projects/RL_DQN_Gym_CliffWalking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a Deep Q Learning Neural Network Agent in Gymnasium's Blackjack Environment"
      ],
      "metadata": {
        "id": "K19cABMcwRRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Installs"
      ],
      "metadata": {
        "id": "hdija3UgzqNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym gym[atari] gym[accept-rom-license] agilerl accelerate>=0.21.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN99d6fxuALj",
        "outputId": "735b748d-d422-4362-82a6-549de92dabf4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: typer 0.12.5 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "### These imports will be used to implement the NN Agent ##\n",
        "import torch\n",
        "from agilerl.algorithms.dqn import DQN\n",
        "from agilerl.components.replay_buffer import ReplayBuffer\n",
        "from agilerl.training.train_off_policy import train_off_policy\n",
        "from agilerl.utils.utils import create_population, make_vect_envs\n",
        "# import trange\n",
        "from tqdm.notebook import trange\n",
        "\n",
        "from tqdm import tqdm\n",
        "from __future__ import annotations\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "g-Pz7XjLsdab"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the Reinforcement Learning Environment"
      ],
      "metadata": {
        "id": "j4YGBETEszni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#env = make_vect_envs(\"CliffWalking-v0\", num_envs=1) # uncomment if want to run across envs\n",
        "env = gym.vector.make(\"CliffWalking-v0\")"
      ],
      "metadata": {
        "id": "bfrlHR2ptyTa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10c29e67-d50a-4eef-e311-da8d1116e45b"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/vector/__init__.py:53: UserWarning: \u001b[33mWARN: `gymnasium.vector.make(...)` is deprecated and will be replaced by `gymnasium.make_vec(...)` in v1.0\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    state_dim = env.single_observation_space.n  # Discrete observation space\n",
        "    one_hot = True  # Requires one-hot encoding\n",
        "    is_discrete_obs = True\n",
        "except Exception:\n",
        "    state_dim = env.single_observation_space.shape  # Continuous observation space\n",
        "    one_hot = False  # Does not require one-hot encoding\n",
        "    is_discrete_obs = False\n",
        "try:\n",
        "    action_dim = env.single_action_space.n  # Discrete action space\n",
        "    is_discrete_actions = True\n",
        "except Exception:\n",
        "    action_dim = env.single_action_space.shape[0]  # Continuous action space\n",
        "    is_discrete_actions = False"
      ],
      "metadata": {
        "id": "Kv0pReTw9E0p"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Action dimension: {action_dim}\")\n",
        "print(f\"Observation dimension: {state_dim}\")\n",
        "print(f\"Is discrete action space: {is_discrete_actions}\")\n",
        "print(f\"Is discrete observation space: {is_discrete_obs}\")\n",
        "print(f\"Is one-hot:  {one_hot}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spBzzW2B9NA1",
        "outputId": "e9c327b6-3a39-4f60-8d0e-a48685be32a2"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action dimension: 4\n",
            "Observation dimension: 48\n",
            "Is discrete action space: True\n",
            "Is discrete observation space: True\n",
            "Is one-hot:  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "oKFhGDjl1Zph"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The blackjack environment does not implement a .shape method on its' observation_space. Since the DQN agent expects a python tuple instead of a gym.tuple, we need to create our own state_dim. Although the output of the above shows it is not discete - the blackjack environment does actually have a discrete observation space."
      ],
      "metadata": {
        "id": "4XNgnCh22Lrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the Deep Q-Learning Agent"
      ],
      "metadata": {
        "id": "6v_HZPW-s6Hs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this block we set the default hyperparameters"
      ],
      "metadata": {
        "id": "EvFa6q8VyEnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial hyperparameters\n",
        "INIT_HP = {\n",
        "    \"BATCH_SIZE\": 64,  # Batch size\n",
        "    \"LR\": 0.0001,  # Learning rate\n",
        "    \"GAMMA\": 0.99,  # Discount factor\n",
        "    \"MEMORY_SIZE\": 5_000,  # Max memory buffer size\n",
        "    \"LEARN_STEP\": 5,  # Learning frequency\n",
        "    \"N_STEP\": 3,  # Step number to calculate td error\n",
        "    \"PER\": True,  # Use prioritized experience replay buffer\n",
        "    \"ALPHA\": 0.6,  # Prioritized replay buffer parameter\n",
        "    \"BETA\": 0.4,  # Importance sampling coefficient\n",
        "    \"TAU\": 0.001,  # For soft update of target parameters\n",
        "    \"PRIOR_EPS\": 0.000001,  # Minimum priority for sampling\n",
        "    \"NUM_ATOMS\": 51,  # Unit number of support\n",
        "    \"V_MIN\": -200.0,  # Minimum value of support\n",
        "    \"V_MAX\": 200.0,  # Maximum value of support\n",
        "    \"NOISY\": True,  # Add noise directly to the weights of the network\n",
        "    # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n",
        "    \"LEARNING_DELAY\": 200,  # Steps before starting learning\n",
        "    \"CHANNELS_LAST\": False,  # Use with RGB states\n",
        "    \"TARGET_SCORE\": 200.0,  # Target score that will beat the environment\n",
        "    \"MAX_STEPS\": 10000,  # Maximum number of steps an agent takes in an environment\n",
        "    \"EVO_STEPS\": 10000,  # Evolution frequency\n",
        "    \"EVAL_STEPS\": 1000,  # Number of evaluation steps per episode\n",
        "    \"EVAL_LOOP\": 1,  # Number of evaluation episodes\n",
        "}"
      ],
      "metadata": {
        "id": "vdhzyK7k2JEe"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this block we set the neural network config. Since we are using the more simple discrete observation and action environment, a typical multi-layer perceptron network is sufficient."
      ],
      "metadata": {
        "id": "Wqw7r5UcyKL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NET_CONFIG = {\n",
        "      'arch': 'mlp',      # Network architecture\n",
        "      'hidden_size': [32, 32],  # Network hidden size\n",
        "}"
      ],
      "metadata": {
        "id": "9oYejbDLyQrQ"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the neural network config is passed to the DQN agent to alter the default network."
      ],
      "metadata": {
        "id": "aDYVi0zLyge-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DQN(\n",
        "    net_config=NET_CONFIG,\n",
        "    batch_size=int(state_dim),\n",
        "    state_dim=[state_dim],\n",
        "    action_dim=action_dim,\n",
        "    one_hot=one_hot,\n",
        "    lr=INIT_HP[\"LR\"],\n",
        "    learn_step=INIT_HP[\"LEARN_STEP\"],\n",
        "    gamma=INIT_HP[\"GAMMA\"],\n",
        "    tau=INIT_HP[\"TAU\"],\n",
        "    device=device)"
      ],
      "metadata": {
        "id": "qdEFmBNbwboI"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the DQN Agent in the Blackjack Environment\n",
        "\n",
        "### What is the difference in training with a DQN agent vs a Q-Learning agent?\n",
        "\n",
        "#### Memory and Replay buffer\n",
        "A difference in training an agent with a memory or replay buffer is that the simpler agent.update(state, action, reward, next_state, done) function is decomposed into multiple functions:\n",
        "\n",
        "1. memory.save_to_memory_vect_envs(state, action, reward, next_state, done)\n",
        "2. experience = memory.sample(agent.batch_size)\n",
        "3. agent.learn(experience)\n",
        "\n",
        "This allows for a higher dimensional input used for training (for example with multiple channels or multiple observations).\n",
        "\n",
        "#### Training steps\n",
        "\n",
        "The training steps when using a memory or replay buffer is dependent on the 'batch_size' of the memory. This determines how many 'experiences' / memory samples (or steps in the environment) the memory should be filled with prior to training. Once the memory is filled (this could be taken as the exploration phase as no learning is taking place), the agent continues taking steps while also learning (the training phase)."
      ],
      "metadata": {
        "id": "eTFu2xEhwcnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the 'Experience Replay Buffer' / agent memory is added to provide learning stability\n",
        "field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]"
      ],
      "metadata": {
        "id": "iry5DfdqtG0y"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ReplayBuffer(memory_size=5000, field_names=field_names, device=device)"
      ],
      "metadata": {
        "id": "MVoJgFRMvTz2"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Exploration params\n",
        "eps_start = 1.0  # Max exploration\n",
        "eps_end = 0.1  # Min exploration\n",
        "eps_decay = 0.995  # Decay per episode\n",
        "epsilon = eps_start"
      ],
      "metadata": {
        "id": "mbkAHGS8Yp5H"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING LOOP\n",
        "total_steps=0\n",
        "pop=[agent]\n",
        "print(\"Training...\")\n",
        "pbar = trange(INIT_HP[\"MAX_STEPS\"], unit=\"step\")\n",
        "while np.less([agent.steps[-1] for agent in pop], INIT_HP[\"MAX_STEPS\"]).all():\n",
        "    pop_episode_scores = []\n",
        "    for agent in pop:  # Loop through population\n",
        "        state, info = env.reset()  # Reset environment at start of episode\n",
        "        scores = np.zeros(1)\n",
        "        completed_episode_scores = []\n",
        "        steps = 0\n",
        "        epsilon = eps_start\n",
        "\n",
        "        for idx_step in range(INIT_HP['MAX_STEPS'] // 1):\n",
        "            if INIT_HP[\"CHANNELS_LAST\"]:\n",
        "                state = np.moveaxis(state, [-1], [-3])\n",
        "\n",
        "            action = agent.get_action(state, epsilon)  # Get next action from agent\n",
        "            epsilon = max(\n",
        "                eps_end, epsilon * eps_decay\n",
        "            )  # Decay epsilon for exploration\n",
        "\n",
        "            # Act in environment\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            scores += np.array(reward)\n",
        "            steps += 1\n",
        "            total_steps += 1\n",
        "\n",
        "            # Collect scores for completed episodes\n",
        "            for idx, (d, t) in enumerate(zip(terminated, truncated)):\n",
        "                if d or t:\n",
        "                    completed_episode_scores.append(scores[idx])\n",
        "                    agent.scores.append(scores[idx])\n",
        "                    scores[idx] = 0\n",
        "\n",
        "            # Save experience to replay buffer\n",
        "            if INIT_HP[\"CHANNELS_LAST\"]:\n",
        "                memory.save_to_memory(\n",
        "                    state,\n",
        "                    action,\n",
        "                    reward,\n",
        "                    np.moveaxis(next_state, [-1], [-3]),\n",
        "                    terminated,\n",
        "                    is_vectorised=True,\n",
        "                )\n",
        "            else:\n",
        "                memory.save_to_memory(\n",
        "                    state,\n",
        "                    action,\n",
        "                    reward,\n",
        "                    next_state,\n",
        "                    terminated,\n",
        "                    is_vectorised=True,\n",
        "                )\n",
        "\n",
        "            # Learn according to learning frequency\n",
        "            if memory.counter > INIT_HP['LEARNING_DELAY'] and len(memory) >= agent.batch_size:\n",
        "                for _ in range(1 // agent.learn_step):\n",
        "                    experiences = memory.sample(\n",
        "                        agent.batch_size\n",
        "                    )  # Sample replay buffer\n",
        "                    agent.learn(\n",
        "                        experiences\n",
        "                    )  # Learn according to agent's RL algorithm\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        pbar.update(INIT_HP['EVO_STEPS'] // len(pop))\n",
        "        agent.steps[-1] += steps\n",
        "        pop_episode_scores.append(completed_episode_scores)\n",
        "\n",
        "    # Reset epsilon start to latest decayed value for next round of population training\n",
        "    eps_start = epsilon\n",
        "\n",
        "    # Evaluate population\n",
        "    fitnesses = [\n",
        "        agent.test(\n",
        "            env,\n",
        "            swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n",
        "            max_steps=INIT_HP['EVAL_STEPS'],\n",
        "            loop=INIT_HP['EVAL_LOOP'],\n",
        "        )\n",
        "        for agent in pop\n",
        "    ]\n",
        "    mean_scores = [\n",
        "        (\n",
        "            np.mean(episode_scores)\n",
        "            if len(episode_scores) > 0\n",
        "            else \"0 completed episodes\"\n",
        "        )\n",
        "        for episode_scores in pop_episode_scores\n",
        "    ]\n",
        "\n",
        "    print(f\"--- Global steps {total_steps} ---\")\n",
        "    print(f\"Steps {[agent.steps[-1] for agent in pop]}\")\n",
        "    print(f\"Scores: {mean_scores}\")\n",
        "    print(f'Fitnesses: {[\"%.2f\"%fitness for fitness in fitnesses]}')\n",
        "    print(\n",
        "        f'5 fitness avgs: {[\"%.2f\"%np.mean(agent.fitness[-5:]) for agent in pop]}'\n",
        "    )\n",
        "\n",
        "    # Update step counter\n",
        "    for agent in pop:\n",
        "        agent.steps.append(agent.steps[-1])\n",
        "\n",
        "pbar.close()\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "91784375546a4f70b6dea96672b9d9b7",
            "0ba85ffc104c433ea4ed6fa601121ebb",
            "2901746b0c1a40dfbab1344b4e5854eb",
            "7f2034df60fa498992a38be765f9b15c",
            "c7f525ac5e65471f9a7c5cae3f5ff73c",
            "ea5a64625959494989cab519b49f22bb",
            "fb38045edf9643f8b7fb522ad013cb10",
            "aed28341c6734cc0a489ae1e5b46247c",
            "c0d5b4cf648a4b748f70ee12a5bf350c",
            "18d4dd9192134d5994ed7cc4edd26994",
            "39180ed07f924558a5668de12b6fd67e"
          ]
        },
        "id": "Im_4OM54YOFR",
        "outputId": "58d6959f-5836-479a-85bc-9c303e27ddb6"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?step/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91784375546a4f70b6dea96672b9d9b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Global steps 10000 ---\n",
            "Steps [10000]\n",
            "Scores: [-499.0]\n",
            "Fitnesses: ['-1000.00']\n",
            "5 fitness avgs: ['-1000.00']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.save_checkpoint('CliffWalking.pt')"
      ],
      "metadata": {
        "id": "bmm89ErCbjZ9"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the Trained Weights and Evaluating the Agent"
      ],
      "metadata": {
        "id": "fo1odlCKV6lC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_env = gym.make(\"CliffWalking-v0\", render_mode='rgb_array')"
      ],
      "metadata": {
        "id": "xl9Az9sNhBEk"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These wrappers are necessary if using the gym.make method instead of the gym.vector.make method to create the test environment. The gym.make method is required if using gym.wrapper.RecordVideo for testing as it sets the render_mode to be able to use rgb_array. The gym.make method forces the environments to use tuples instead of arrays for the data types for observations or actions so is incompatible with the DQN class of AgileRL which expects arrays and not tuples."
      ],
      "metadata": {
        "id": "DJzdQ1I_vJzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ArrayObservationEnv(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "    # code to overwrite the step and reset functions to modify the state\n",
        "\n",
        "    #super() of env.step\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        #return tuple([obs]), reward, terminated, truncated, info\n",
        "        return np.array(obs), reward, terminated, truncated, info\n",
        "\n",
        "    #super() of env.reset\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        #return tuple([obs]), info\n",
        "        return np.array(obs), info\n",
        "\n",
        "\n",
        "#wrapper for agileRL DQN\n",
        "\n",
        "class TupletoArrayDQN():\n",
        "    def __init__(self, trained_agent):  # Pass keyword arguments for DQN initialization\n",
        "        self.dqn_instance = trained_agent\n",
        "        #super().__init__() # Added this to solve NameError, not sure if necessary but it works now\n",
        "\n",
        "    #super() of DQN.test\n",
        "    def test(self, env, swap_channels=False, max_steps=None, loop=1):\n",
        "        # uses array env wrapper\n",
        "        env = ArrayObservationEnv(env)\n",
        "        \"\"\"Returns mean test score of agent in environment with epsilon-greedy policy.\n",
        "\n",
        "        :param env: The environment to be tested in\n",
        "        :type env: Gym-style environment\n",
        "        :param swap_channels: Swap image channels dimension from last to first [H, W, C] -> [C, H, W], defaults to False\n",
        "        :type swap_channels: bool, optional\n",
        "        :param max_steps: Maximum number of testing steps, defaults to None\n",
        "        :type max_steps: int, optional\n",
        "        :param loop: Number of testing loops/episodes to complete. The returned score is the mean over these tests. Defaults to 1\n",
        "        :type loop: int, optional\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            rewards = []\n",
        "            num_envs = env.num_envs if hasattr(env, \"num_envs\") else 1\n",
        "            for i in range(loop):\n",
        "                state, info = env.reset()\n",
        "                scores = np.zeros(num_envs)\n",
        "                completed_episode_scores = np.zeros(num_envs)\n",
        "                finished = np.zeros(num_envs)\n",
        "                step = 0\n",
        "                while not np.all(finished):\n",
        "                    if swap_channels:\n",
        "                        state = np.moveaxis(state, [-1], [-3])\n",
        "                    action_mask = info.get(\"action_mask\", None)\n",
        "                    action = self.dqn_instance.get_action(state, epsilon=0, action_mask=action_mask)\n",
        "                    state, reward, done, trunc, info = env.step(action[0])\n",
        "                    step += 1\n",
        "                    scores += np.array(reward)\n",
        "                    for idx, (d, t) in enumerate(zip([done], [trunc])):\n",
        "                        if (\n",
        "                            d or t or (max_steps is not None and step == max_steps)\n",
        "                        ) and not finished[idx]:\n",
        "                            completed_episode_scores[idx] = scores[idx]\n",
        "                            finished[idx] = 1\n",
        "                rewards.append(np.mean(completed_episode_scores))\n",
        "        mean_fit = np.mean(rewards)\n",
        "        self.dqn_instance.fitness.append(mean_fit)\n",
        "        return mean_fit\n",
        "\n"
      ],
      "metadata": {
        "id": "NWH4DdKhkCpJ"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_dqn = TupletoArrayDQN(trained_agent=agent)"
      ],
      "metadata": {
        "id": "I4KIZGBMnKjQ"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uses the Gym Monitor wrapper to evalaute the agent and record video\n",
        "# only one video will be saved\n",
        "# video of the final episode with the episode trigger\n",
        "test_env = gym.wrappers.RecordVideo(\n",
        "    test_env, \"./gym_monitor_output\", episode_trigger=lambda x: x == 0)\n",
        "\n",
        "wrapped_dqn.test(test_env, swap_channels=INIT_HP[\"CHANNELS_LAST\"], max_steps=INIT_HP['EVAL_STEPS'])\n",
        "\n",
        "test_env.close()"
      ],
      "metadata": {
        "id": "HJPj-rXDtlHX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60c8393b-e030-401f-8468-6d38fb37e150"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video /content/gym_monitor_output/rl-video-episode-0.mp4.\n",
            "Moviepy - Writing video /content/gym_monitor_output/rl-video-episode-0.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/gym_monitor_output/rl-video-episode-0.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i56hTzBghzAs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}