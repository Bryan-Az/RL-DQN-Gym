{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bryan-Az/RL-DQN-Gym/blob/main/DQN_Projects/RL_DQN_Gym_CliffWalking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a Deep Q Learning Neural Network Agent in Gymnasium's Blackjack Environment"
      ],
      "metadata": {
        "id": "K19cABMcwRRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Installs"
      ],
      "metadata": {
        "id": "hdija3UgzqNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym gym[atari] gym[accept-rom-license] agilerl accelerate>=0.21.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN99d6fxuALj",
        "outputId": "c839b522-13bd-41c9-b213-ba2a9df64868"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: typer 0.12.5 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "### These imports will be used to implement the NN Agent ##\n",
        "import torch\n",
        "from agilerl.algorithms.dqn import DQN\n",
        "from agilerl.components.replay_buffer import ReplayBuffer\n",
        "from agilerl.training.train_off_policy import train_off_policy\n",
        "from agilerl.utils.utils import create_population, make_vect_envs\n",
        "\n",
        "from tqdm import tqdm\n",
        "from __future__ import annotations\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "g-Pz7XjLsdab"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the Reinforcement Learning Environment"
      ],
      "metadata": {
        "id": "j4YGBETEszni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = make_vect_envs(\"CliffWalking-v0\", num_envs=1)\n",
        "n_episodes = 1_000\n",
        "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)"
      ],
      "metadata": {
        "id": "bfrlHR2ptyTa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    state_dim = env.single_observation_space.n  # Discrete observation space\n",
        "    one_hot = True  # Requires one-hot encoding\n",
        "    is_discrete_obs = True\n",
        "except Exception:\n",
        "    state_dim = env.single_observation_space.shape  # Continuous observation space\n",
        "    one_hot = False  # Does not require one-hot encoding\n",
        "    is_discrete_obs = False\n",
        "try:\n",
        "    action_dim = env.single_action_space.n  # Discrete action space\n",
        "    is_discrete_actions = True\n",
        "except Exception:\n",
        "    action_dim = env.single_action_space.shape[0]  # Continuous action space\n",
        "    is_discrete_actions = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kv0pReTw9E0p",
        "outputId": "84f71f0f-9a1b-4eb8-cc23-30719225283d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_observation_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_observation_space` for environment variables or `env.get_wrapper_attr('single_observation_space')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_action_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_action_space` for environment variables or `env.get_wrapper_attr('single_action_space')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Action dimension: {action_dim}\")\n",
        "print(f\"Observation dimension: {state_dim}\")\n",
        "print(f\"Is discrete action space: {is_discrete_actions}\")\n",
        "print(f\"Is discrete observation space: {is_discrete_obs}\")\n",
        "print(f\"Is one-hot:  {one_hot}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spBzzW2B9NA1",
        "outputId": "f8b0abc3-8469-49a5-9920-ed8330c2157a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action dimension: 4\n",
            "Observation dimension: 48\n",
            "Is discrete action space: True\n",
            "Is discrete observation space: True\n",
            "Is one-hot:  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "oKFhGDjl1Zph"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The blackjack environment does not implement a .shape method on its' observation_space. Since the DQN agent expects a python tuple instead of a gym.tuple, we need to create our own state_dim. Although the output of the above shows it is not discete - the blackjack environment does actually have a discrete observation space."
      ],
      "metadata": {
        "id": "4XNgnCh22Lrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the Deep Q-Learning Agent"
      ],
      "metadata": {
        "id": "6v_HZPW-s6Hs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this block we set the default hyperparameters"
      ],
      "metadata": {
        "id": "EvFa6q8VyEnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial hyperparameters\n",
        "INIT_HP = {\n",
        "    \"BATCH_SIZE\": 64,  # Batch size\n",
        "    \"LR\": 0.0001,  # Learning rate\n",
        "    \"GAMMA\": 0.99,  # Discount factor\n",
        "    \"MEMORY_SIZE\": 5_000,  # Max memory buffer size\n",
        "    \"LEARN_STEP\": 5,  # Learning frequency\n",
        "    \"N_STEP\": 3,  # Step number to calculate td error\n",
        "    \"PER\": True,  # Use prioritized experience replay buffer\n",
        "    \"ALPHA\": 0.6,  # Prioritized replay buffer parameter\n",
        "    \"BETA\": 0.4,  # Importance sampling coefficient\n",
        "    \"TAU\": 0.001,  # For soft update of target parameters\n",
        "    \"PRIOR_EPS\": 0.000001,  # Minimum priority for sampling\n",
        "    \"NUM_ATOMS\": 51,  # Unit number of support\n",
        "    \"V_MIN\": -200.0,  # Minimum value of support\n",
        "    \"V_MAX\": 200.0,  # Maximum value of support\n",
        "    \"NOISY\": True,  # Add noise directly to the weights of the network\n",
        "    # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n",
        "    \"LEARNING_DELAY\": 5000,  # Steps before starting learning\n",
        "    \"CHANNELS_LAST\": False,  # Use with RGB states\n",
        "    \"TARGET_SCORE\": 200.0,  # Target score that will beat the environment\n",
        "    \"MAX_STEPS\": 10000,  # Maximum number of steps an agent takes in an environment\n",
        "    \"EVO_STEPS\": 10000,  # Evolution frequency\n",
        "    \"EVAL_STEPS\": None,  # Number of evaluation steps per episode\n",
        "    \"EVAL_LOOP\": 1,  # Number of evaluation episodes\n",
        "}"
      ],
      "metadata": {
        "id": "vdhzyK7k2JEe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this block we set the neural network config. Since we are using the more simple discrete observation and action environment, a typical multi-layer perceptron network is sufficient."
      ],
      "metadata": {
        "id": "Wqw7r5UcyKL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NET_CONFIG = {\n",
        "      'arch': 'mlp',      # Network architecture\n",
        "      'hidden_size': [32, 32],  # Network hidden size\n",
        "}"
      ],
      "metadata": {
        "id": "9oYejbDLyQrQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the neural network config is passed to the DQN agent to alter the default network."
      ],
      "metadata": {
        "id": "aDYVi0zLyge-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DQN(\n",
        "    net_config=NET_CONFIG,\n",
        "    batch_size=int(state_dim),\n",
        "    state_dim=[state_dim],\n",
        "    action_dim=action_dim,\n",
        "    one_hot=one_hot,\n",
        "    lr=INIT_HP[\"LR\"],\n",
        "    learn_step=INIT_HP[\"LEARN_STEP\"],\n",
        "    gamma=INIT_HP[\"GAMMA\"],\n",
        "    tau=INIT_HP[\"TAU\"],\n",
        "    device=device)"
      ],
      "metadata": {
        "id": "qdEFmBNbwboI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the DQN Agent in the Blackjack Environment\n",
        "\n",
        "### What is the difference in training with a DQN agent vs a Q-Learning agent?\n",
        "\n",
        "#### Memory and Replay buffer\n",
        "A difference in training an agent with a memory or replay buffer is that the simpler agent.update(state, action, reward, next_state, done) function is decomposed into multiple functions:\n",
        "\n",
        "1. memory.save_to_memory_vect_envs(state, action, reward, next_state, done)\n",
        "2. experience = memory.sample(agent.batch_size)\n",
        "3. agent.learn(experience)\n",
        "\n",
        "This allows for a higher dimensional input used for training (for example with multiple channels or multiple observations).\n",
        "\n",
        "#### Training steps\n",
        "\n",
        "The training steps when using a memory or replay buffer is dependent on the 'batch_size' of the memory. This determines how many 'experiences' / memory samples (or steps in the environment) the memory should be filled with prior to training. Once the memory is filled (this could be taken as the exploration phase as no learning is taking place), the agent continues taking steps while also learning (the training phase)."
      ],
      "metadata": {
        "id": "eTFu2xEhwcnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the environmental variables are inferred for use with the agent\n",
        "# the 'Experience Replay Buffer' / agent memory is added to provide learning stability\n",
        "\n",
        "field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]"
      ],
      "metadata": {
        "id": "iry5DfdqtG0y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ReplayBuffer(memory_size=5000, field_names=field_names, device=device)\n",
        "trained_pop, pop_fitnesses = train_off_policy(\n",
        "    env=env,\n",
        "    env_name=\"CliffWalking-v0\",\n",
        "    algo=\"DQN\",\n",
        "    memory=memory,\n",
        "    pop=[agent],\n",
        "    target=INIT_HP[\"TARGET_SCORE\"],\n",
        "    max_steps=INIT_HP[\"MAX_STEPS\"],\n",
        "    learning_delay=INIT_HP[\"LEARNING_DELAY\"],\n",
        "    tournament=None,\n",
        "    mutation=None,\n",
        "    wb=False,  # Boolean flag to record run with Weights & Biases\n",
        "    checkpoint=INIT_HP[\"MAX_STEPS\"],\n",
        "    checkpoint_path=\"DQN.pt\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVoJgFRMvTz2",
        "outputId": "a91426ee-8279-415e-ae0a-a2c0cbe98c94"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          |    0/10000 [  00:00<      ?, ?step/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the Trained Weights and Evaluating the Agent"
      ],
      "metadata": {
        "id": "fo1odlCKV6lC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"DQN.pt\"\n",
        "agent.save_checkpoint(checkpoint_path)"
      ],
      "metadata": {
        "id": "jp8leLoeV_YZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uses the Gym Monitor wrapper to evalaute the agent and record video\n",
        "# only one video will be saved\n",
        "\n",
        "# video of the final episode with the episode trigger\n",
        "#env = gym.wrappers.RecordVideo(\n",
        "    env, \"./gym_monitor_output\", episode_trigger=lambda x: x == 0)\n",
        "\n",
        "#test(agent, env, learned_policy)\n",
        "\n",
        "#env.close()"
      ],
      "metadata": {
        "id": "HJPj-rXDtlHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hQhfvfJb0x6a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}